{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc51e64-384e-484e-9a8e-26d5c5177eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from debug_model_comparison import debug_model_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288f4786-a904-4ddc-9623-a04189d71343",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_models(\n",
    "        model1_path=None,  # Replace with path to Script 1 checkpoint\n",
    "        model2_path=None,  # Replace with path to Script 2 checkpoint\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d35ce43d-4dfb-4186-b8fa-479517c9a7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alok/Desktop/antispoof_offline\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c49670ff-41f2-4044-bced-927868cb60b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TESTING MODEL: maxvit_tiny_tf_224\n",
      "====================================================================================================\n",
      "================================================================================\n",
      "MODEL COMPARISON - CREATING FROM SCRATCH\n",
      "================================================================================\n",
      "\n",
      "Device: cpu\n",
      "Model: maxvit_tiny_tf_224\n",
      "\n",
      "==================================================\n",
      "CREATING SCRIPT 1 MODEL\n",
      "==================================================\n",
      "\n",
      "[Script 1] Creating model: maxvit_tiny_tf_224\n",
      "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124M/124M [00:09<00:00, 13.0MB/s]\n",
      "[Script 1] Backbone created with num_classes=0\n",
      "[Script 1] Backbone output dimension: 512\n",
      "[Script 1] Model construction complete\n",
      "\n",
      "==================================================\n",
      "CREATING SCRIPT 2 MODEL\n",
      "==================================================\n",
      "[Script 2] Simulating ModelManager.load_or_download_model()\n",
      "\n",
      "[Script 2] Creating model: maxvit_tiny_tf_224\n",
      "[Script 2] Initializing with backbone\n",
      "[Script 2] Used forward_features, output shape: torch.Size([2, 512, 7, 7])\n",
      "[Script 2] Final backbone dimension: 512\n",
      "[Script 2] Model initialization complete\n",
      "\n",
      "================================================================================\n",
      "DETAILED MODEL COMPARISON\n",
      "================================================================================\n",
      "\n",
      "1. PARAMETER COUNT\n",
      "----------------------------------------\n",
      "Script 1:\n",
      "  Total: 30,667,210\n",
      "  Trainable: 30,667,210\n",
      "  Frozen: 0\n",
      "\n",
      "Script 2:\n",
      "  Total: 31,180,210\n",
      "  Trainable: 31,180,210\n",
      "  Frozen: 0\n",
      "\n",
      "Difference:\n",
      "  Total: 513,000 âœ—\n",
      "  Trainable: 513,000 âœ—\n",
      "  Frozen: 0 âœ“\n",
      "\n",
      "By Component:\n",
      "\n",
      "  backbone:\n",
      "    Script 1: 30,403,528 total, 30,403,528 trainable\n",
      "    Script 2: 30,916,528 total, 30,916,528 trainable\n",
      "    MISMATCH! âœ—\n",
      "\n",
      "  classifier:\n",
      "    Script 1: 1,026 total, 1,026 trainable\n",
      "    Script 2: 1,026 total, 1,026 trainable\n",
      "\n",
      "  feature_projection:\n",
      "    Script 1: 262,656 total, 262,656 trainable\n",
      "    Script 2: 262,656 total, 262,656 trainable\n",
      "\n",
      "2. LAYER NAME COMPARISON\n",
      "----------------------------------------\n",
      "\n",
      "Layers only in Script 2 (2):\n",
      "  - backbone.head.fc.bias\n",
      "  - backbone.head.fc.weight\n",
      "\n",
      "3. REQUIRES_GRAD STATUS\n",
      "----------------------------------------\n",
      "âœ“ All common parameters have same requires_grad status\n",
      "\n",
      "4. FORWARD PASS TEST\n",
      "----------------------------------------\n",
      "Script 1: Success\n",
      "  Logits shape: torch.Size([2, 2])\n",
      "  Features shape: torch.Size([2, 512])\n",
      "Script 2: Success\n",
      "  Logits shape: torch.Size([2, 2])\n",
      "  Features shape: torch.Size([2, 512])\n",
      "\n",
      "5. BACKBONE ANALYSIS\n",
      "----------------------------------------\n",
      "Script 1 backbone type: MaxxVit\n",
      "Script 2 backbone type: MaxxVit\n",
      "\n",
      "Backbone parameters:\n",
      "  Script 1: 30,403,528\n",
      "  Script 2: 30,916,528\n",
      "  Difference: 513,000\n",
      "\n",
      "Script 1 backbone.head: NormMlpClassifierHead(\n",
      "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
      "  (norm): LayerNorm2d((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (pre_logits): Sequential(\n",
      "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (act): Tanh()\n",
      "  )\n",
      "  (drop): Dropout(p=0.0, inplace=False)\n",
      "  (fc): Identity()\n",
      ")\n",
      "Script 2 backbone.head: NormMlpClassifierHead(\n",
      "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
      "  (norm): LayerNorm2d((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (pre_logits): Sequential(\n",
      "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (act): Tanh()\n",
      "  )\n",
      "  (drop): Dropout(p=0.0, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "\n",
      "6. GRADIENT FLOW TEST\n",
      "----------------------------------------\n",
      "\n",
      "Script 1 gradient flow:\n",
      "  backbone: 456/456 (100%) âœ“\n",
      "  feature_projection: 2/2 (100%) âœ“\n",
      "  classifier: 2/2 (100%) âœ“\n",
      "\n",
      "Script 2 gradient flow:\n",
      "  backbone: 452/458 (99%) âœ—\n",
      "  feature_projection: 2/2 (100%) âœ“\n",
      "  classifier: 2/2 (100%) âœ“\n",
      "\n",
      "================================================================================\n",
      "COMPARISON COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "!python compare_models_from_scratch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0f7c584-eb7c-4f49-8dfe-01e67be0911a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alok/Desktop/antispoof_offline\n"
     ]
    }
   ],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa415d0-907b-4a3c-b6a7-34772abd8396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stage2_training_old import Stage2Trainer as trainer1\n",
    "from stage2_training import Stage2Trainer as trainer2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d251ddc-c74e-4418-86a0-8859120ef2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ NetEase Trainer initialized\n",
      "ðŸ“ Data root: .\n",
      "ðŸ’¾ Output dir: .\n",
      "ðŸ—„ï¸  Model cache: ./model\n",
      "ðŸ”§ Device: cpu\n",
      "âš ï¸  No standard splits found. Available: ['stage2_training_old.py', 'model_tests.ipynb', 'dataAnalyser.ipynb', 'jupyter_netease_trainer.py', 'model_utils.py', 'complete_file_structure.md', 'stage1_training_old.py', 'logs', 'jupyter_netease_trainer (copy).py', 'wfas_dataset_analyzer.py', 'DataSplit.ipynb', 'Untitled.ipynb', 'netease_training_notebook.py', '.git', 'debugModels.ipynb', 'dataset_analysis', 'dataset_merger.py', 'compare_models_from_scratch.py', 'debug_model_comparison.py', 'checkpoints', 'netease_experiment_1', 'anaconda_projects', 'run_netease_pipeline.py', '.ipynb_checkpoints', 'training.ipynb', 'stage1_training.py', 'model', 'dataset_splitter.py', '__pycache__', '__init__.py', 'stage2_training.py', 'model_manager.py', 'wfas_dataset.py']\n",
      "ðŸ“¦ Cached models:\n",
      "  stage2: maxvit_base_tf_224\n",
      "  stage1: convnext_base, convnextv2_base\n"
     ]
    }
   ],
   "source": [
    "from jupyter_netease_trainer import JupyterNeteaseTrainer\n",
    "trainer = JupyterNeteaseTrainer('.', '.')\n",
    "config1 = trainer.create_stage2_config('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4989d6c6-bfe6-4888-b0bf-748e20b5ebdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 18:23:29,337 - INFO - Loading pretrained weights from Hugging Face hub (timm/maxvit_base_tf_224.in1k)\n",
      "2025-06-27 18:23:29,654 - INFO - [timm/maxvit_base_tf_224.in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2025-06-27 18:23:31,024 - INFO - Total parameters: 119,093,462\n",
      "2025-06-27 18:23:31,024 - INFO - Trainable parameters: 119,093,462\n",
      "2025-06-27 18:23:31,025 - INFO - Loading model from local directory: ./model/stage2/maxvit_base_tf_224\n",
      "2025-06-27 18:23:32,999 - INFO - Loaded model weights from: ./model/stage2/maxvit_base_tf_224/pytorch_model.bin\n",
      "2025-06-27 18:23:33,015 - INFO - Successfully loaded model from local cache\n",
      "2025-06-27 18:23:34,031 - INFO - Total parameters: 119,093,462\n",
      "2025-06-27 18:23:34,032 - INFO - Trainable parameters: 119,093,462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used backbone forward, output shape: torch.Size([2, 768])\n",
      "Final backbone dimension: 768\n"
     ]
    }
   ],
   "source": [
    "t1= trainer1(config1)\n",
    "t2= trainer2(config1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45cb2513-d8cc-44b0-9772-4ef02e8a86e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = t1.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "568dd9b7-102b-490d-a649-bda3a96c23fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = t2.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23590726-2a50-4f3c-a654-ca7e75f9ac5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script 1:\n",
      "  Total: 119,093,462\n",
      "  Trainable: 119,093,462\n",
      "  Frozen: 0\n",
      "\n",
      "Script 2:\n",
      "  Total: 119,093,462\n",
      "  Trainable: 119,093,462\n",
      "  Frozen: 0\n",
      "\n",
      "Difference:\n",
      "  Total: 0 âœ“\n",
      "  Trainable: 0 âœ“\n",
      "  Frozen: 0 âœ“\n",
      "\n",
      "By Component:\n",
      "\n",
      "  backbone:\n",
      "    Script 1: 118,698,708 total, 118,698,708 trainable\n",
      "    Script 2: 118,698,708 total, 118,698,708 trainable\n",
      "\n",
      "  classifier:\n",
      "    Script 1: 1,026 total, 1,026 trainable\n",
      "    Script 2: 1,026 total, 1,026 trainable\n",
      "\n",
      "  feature_projection:\n",
      "    Script 1: 393,728 total, 393,728 trainable\n",
      "    Script 2: 393,728 total, 393,728 trainable\n",
      "\n",
      "2. LAYER NAME COMPARISON\n",
      "----------------------------------------\n",
      "âœ“ Both models have identical layer names\n",
      "\n",
      "3. REQUIRES_GRAD STATUS\n",
      "----------------------------------------\n",
      "âœ“ All common parameters have same requires_grad status\n",
      "\n",
      "4. FORWARD PASS TEST\n",
      "----------------------------------------\n",
      "Script 1: Success\n",
      "  Logits shape: torch.Size([2, 2])\n",
      "  Features shape: torch.Size([2, 512])\n",
      "Script 2: Success\n",
      "  Logits shape: torch.Size([2, 2])\n",
      "  Features shape: torch.Size([2, 512])\n",
      "\n",
      "5. BACKBONE ANALYSIS\n",
      "----------------------------------------\n",
      "Script 1 backbone type: MaxxVit\n",
      "Script 2 backbone type: MaxxVit\n",
      "\n",
      "Backbone parameters:\n",
      "  Script 1: 118,698,708\n",
      "  Script 2: 118,698,708\n",
      "  Difference: 0\n",
      "\n",
      "Script 1 backbone.head: NormMlpClassifierHead(\n",
      "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
      "  (norm): LayerNorm2d((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (pre_logits): Sequential(\n",
      "    (fc): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (act): Tanh()\n",
      "  )\n",
      "  (drop): Dropout(p=0.0, inplace=False)\n",
      "  (fc): Identity()\n",
      ")\n",
      "Script 2 backbone.head: NormMlpClassifierHead(\n",
      "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
      "  (norm): LayerNorm2d((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (pre_logits): Sequential(\n",
      "    (fc): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (act): Tanh()\n",
      "  )\n",
      "  (drop): Dropout(p=0.0, inplace=False)\n",
      "  (fc): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "def count_parameters_detailed(model, name):\n",
    "    results = {\n",
    "        'total': 0,\n",
    "        'trainable': 0,\n",
    "        'frozen': 0,\n",
    "        'by_component': {}\n",
    "    }\n",
    "    \n",
    "    for param_name, param in model.named_parameters():\n",
    "        component = param_name.split('.')[0]\n",
    "        if component not in results['by_component']:\n",
    "            results['by_component'][component] = {\n",
    "                'total': 0,\n",
    "                'trainable': 0,\n",
    "                'frozen': 0\n",
    "            }\n",
    "        \n",
    "        param_count = param.numel()\n",
    "        results['total'] += param_count\n",
    "        results['by_component'][component]['total'] += param_count\n",
    "        \n",
    "        if param.requires_grad:\n",
    "            results['trainable'] += param_count\n",
    "            results['by_component'][component]['trainable'] += param_count\n",
    "        else:\n",
    "            results['frozen'] += param_count\n",
    "            results['by_component'][component]['frozen'] += param_count\n",
    "    \n",
    "    return results\n",
    "\n",
    "stats1 = count_parameters_detailed(model1, \"Script 1\")\n",
    "stats2 = count_parameters_detailed(model2, \"Script 2\")\n",
    "\n",
    "print(f\"Script 1:\")\n",
    "print(f\"  Total: {stats1['total']:,}\")\n",
    "print(f\"  Trainable: {stats1['trainable']:,}\")\n",
    "print(f\"  Frozen: {stats1['frozen']:,}\")\n",
    "\n",
    "print(f\"\\nScript 2:\")\n",
    "print(f\"  Total: {stats2['total']:,}\")\n",
    "print(f\"  Trainable: {stats2['trainable']:,}\")\n",
    "print(f\"  Frozen: {stats2['frozen']:,}\")\n",
    "\n",
    "print(f\"\\nDifference:\")\n",
    "print(f\"  Total: {stats2['total'] - stats1['total']:,} {'âœ“' if stats2['total'] == stats1['total'] else 'âœ—'}\")\n",
    "print(f\"  Trainable: {stats2['trainable'] - stats1['trainable']:,} {'âœ“' if stats2['trainable'] == stats1['trainable'] else 'âœ—'}\")\n",
    "print(f\"  Frozen: {stats2['frozen'] - stats1['frozen']:,} {'âœ“' if stats2['frozen'] == stats1['frozen'] else 'âœ—'}\")\n",
    "\n",
    "# Component breakdown\n",
    "print(\"\\nBy Component:\")\n",
    "all_components = set(stats1['by_component'].keys()) | set(stats2['by_component'].keys())\n",
    "for component in sorted(all_components):\n",
    "    s1 = stats1['by_component'].get(component, {'total': 0, 'trainable': 0})\n",
    "    s2 = stats2['by_component'].get(component, {'total': 0, 'trainable': 0})\n",
    "    \n",
    "    print(f\"\\n  {component}:\")\n",
    "    print(f\"    Script 1: {s1['total']:,} total, {s1['trainable']:,} trainable\")\n",
    "    print(f\"    Script 2: {s2['total']:,} total, {s2['trainable']:,} trainable\")\n",
    "    if s1['total'] != s2['total'] or s1['trainable'] != s2['trainable']:\n",
    "        print(f\"    MISMATCH! âœ—\")\n",
    "\n",
    "# 2. Layer name comparison\n",
    "print(\"\\n2. LAYER NAME COMPARISON\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "params1_names = set(dict(model1.named_parameters()).keys())\n",
    "params2_names = set(dict(model2.named_parameters()).keys())\n",
    "\n",
    "only_in_1 = params1_names - params2_names\n",
    "only_in_2 = params2_names - params1_names\n",
    "\n",
    "if only_in_1:\n",
    "    print(f\"\\nLayers only in Script 1 ({len(only_in_1)}):\")\n",
    "    for name in sorted(only_in_1)[:5]:  # Show first 5\n",
    "        print(f\"  - {name}\")\n",
    "    if len(only_in_1) > 5:\n",
    "        print(f\"  ... and {len(only_in_1) - 5} more\")\n",
    "\n",
    "if only_in_2:\n",
    "    print(f\"\\nLayers only in Script 2 ({len(only_in_2)}):\")\n",
    "    for name in sorted(only_in_2)[:5]:  # Show first 5\n",
    "        print(f\"  - {name}\")\n",
    "    if len(only_in_2) > 5:\n",
    "        print(f\"  ... and {len(only_in_2) - 5} more\")\n",
    "\n",
    "if not only_in_1 and not only_in_2:\n",
    "    print(\"âœ“ Both models have identical layer names\")\n",
    "\n",
    "# 3. Check requires_grad status\n",
    "print(\"\\n3. REQUIRES_GRAD STATUS\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "common_params = params1_names & params2_names\n",
    "grad_mismatches = []\n",
    "\n",
    "params1_dict = dict(model1.named_parameters())\n",
    "params2_dict = dict(model2.named_parameters())\n",
    "\n",
    "for param_name in common_params:\n",
    "    grad1 = params1_dict[param_name].requires_grad\n",
    "    grad2 = params2_dict[param_name].requires_grad\n",
    "    \n",
    "    if grad1 != grad2:\n",
    "        grad_mismatches.append((param_name, grad1, grad2))\n",
    "\n",
    "if grad_mismatches:\n",
    "    print(f\"Found {len(grad_mismatches)} parameters with different requires_grad:\")\n",
    "    for name, grad1, grad2 in grad_mismatches[:5]:\n",
    "        print(f\"  {name}: Script1={grad1}, Script2={grad2}\")\n",
    "    if len(grad_mismatches) > 5:\n",
    "        print(f\"  ... and {len(grad_mismatches) - 5} more\")\n",
    "else:\n",
    "    print(\"âœ“ All common parameters have same requires_grad status\")\n",
    "\n",
    "# 4. Test forward pass\n",
    "print(\"\\n4. FORWARD PASS TEST\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_input = torch.randn(2, 3, 224, 224).to(device)\n",
    "    \n",
    "    try:\n",
    "        logits1, features1 = model1(test_input)\n",
    "        print(f\"Script 1: Success\")\n",
    "        print(f\"  Logits shape: {logits1.shape}\")\n",
    "        print(f\"  Features shape: {features1.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Script 1: FAILED - {e}\")\n",
    "    \n",
    "    try:\n",
    "        logits2, features2 = model2(test_input)\n",
    "        print(f\"Script 2: Success\")\n",
    "        print(f\"  Logits shape: {logits2.shape}\")\n",
    "        print(f\"  Features shape: {features2.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Script 2: FAILED - {e}\")\n",
    "\n",
    "# 5. Check backbone specifics\n",
    "print(\"\\n5. BACKBONE ANALYSIS\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "if hasattr(model1, 'backbone') and hasattr(model2, 'backbone'):\n",
    "    print(f\"Script 1 backbone type: {type(model1.backbone).__name__}\")\n",
    "    print(f\"Script 2 backbone type: {type(model2.backbone).__name__}\")\n",
    "    \n",
    "    # Check if backbones have different number of parameters\n",
    "    backbone1_params = sum(p.numel() for p in model1.backbone.parameters())\n",
    "    backbone2_params = sum(p.numel() for p in model2.backbone.parameters())\n",
    "    \n",
    "    print(f\"\\nBackbone parameters:\")\n",
    "    print(f\"  Script 1: {backbone1_params:,}\")\n",
    "    print(f\"  Script 2: {backbone2_params:,}\")\n",
    "    print(f\"  Difference: {backbone2_params - backbone1_params:,}\")\n",
    "    \n",
    "    # Check if backbone has a head/classifier\n",
    "    if hasattr(model1.backbone, 'head'):\n",
    "        print(f\"\\nScript 1 backbone.head: {model1.backbone.head}\")\n",
    "    if hasattr(model2.backbone, 'head'):\n",
    "        print(f\"Script 2 backbone.head: {model2.backbone.head}\")\n",
    "        \n",
    "    if hasattr(model1.backbone, 'classifier'):\n",
    "        print(f\"\\nScript 1 backbone.classifier: {model1.backbone.classifier}\")\n",
    "    if hasattr(model2.backbone, 'classifier'):\n",
    "        print(f\"Script 2 backbone.classifier: {model2.backbone.classifier}\")\n",
    "\n",
    "# # 6. Training test\n",
    "# print(\"\\n6. GRADIENT FLOW TEST\")\n",
    "# print(\"-\"*40)\n",
    "\n",
    "# model1.train()\n",
    "# model2.train()\n",
    "\n",
    "# # Simple gradient test\n",
    "# test_input = torch.randn(4, 3, 224, 224).to(device)\n",
    "# test_labels = torch.randint(0, 2, (4,)).to(device)\n",
    "\n",
    "# for model_name, model in [(\"Script 1\", model1), (\"Script 2\", model2)]:\n",
    "#     model.zero_grad()\n",
    "    \n",
    "#     logits, features = model(test_input)\n",
    "#     loss = F.cross_entropy(logits, test_labels)\n",
    "#     loss.backward()\n",
    "    \n",
    "#     # Check which components have gradients\n",
    "#     components_with_grad = {}\n",
    "#     for name, param in model.named_parameters():\n",
    "#         component = name.split('.')[0]\n",
    "#         if component not in components_with_grad:\n",
    "#             components_with_grad[component] = {'total': 0, 'with_grad': 0}\n",
    "        \n",
    "#         components_with_grad[component]['total'] += 1\n",
    "#         if param.grad is not None and param.grad.abs().sum() > 0:\n",
    "#             components_with_grad[component]['with_grad'] += 1\n",
    "    \n",
    "#     print(f\"\\n{model_name} gradient flow:\")\n",
    "#     for component, stats in components_with_grad.items():\n",
    "#         ratio = stats['with_grad'] / stats['total'] * 100\n",
    "#         status = 'âœ“' if ratio == 100 else 'âœ—'\n",
    "#         print(f\"  {component}: {stats['with_grad']}/{stats['total']} ({ratio:.0f}%) {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "315d826e-d0d9-48c1-b6ee-4acb083d026e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 17:34:13,803 - INFO - Loading pretrained weights from Hugging Face hub (timm/maxvit_base_tf_224.in1k)\n",
      "2025-06-27 17:34:14,161 - INFO - [timm/maxvit_base_tf_224.in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "model = timm.create_model(model_name='maxvit_base_tf_224', pretrained=True, num_classes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ac711c1-19ab-44bf-a8ce-da69305ed0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaxxVit(\n",
       "  (stem): Stem(\n",
       "    (conv1): Conv2dSame(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (norm1): BatchNormAct2d(\n",
       "      64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "      (drop): Identity()\n",
       "      (act): GELUTanh()\n",
       "    )\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (stages): Sequential(\n",
       "    (0): MaxxVitStage(\n",
       "      (blocks): Sequential(\n",
       "        (0): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Downsample2d(\n",
       "              (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "              (expand): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2dSame(384, 384, kernel_size=(3, 3), stride=(2, 2), groups=384, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): MaxxVitStage(\n",
       "      (blocks): Sequential(\n",
       "        (0): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Downsample2d(\n",
       "              (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "              (expand): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(96, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2dSame(768, 768, kernel_size=(3, 3), stride=(2, 2), groups=768, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (2): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (3): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (4): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (5): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): MaxxVitStage(\n",
       "      (blocks): Sequential(\n",
       "        (0): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Downsample2d(\n",
       "              (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "              (expand): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(192, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2dSame(1536, 1536, kernel_size=(3, 3), stride=(2, 2), groups=1536, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (2): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (3): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (4): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (5): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (6): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (7): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (8): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (9): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (10): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (11): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (12): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (13): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): MaxxVitStage(\n",
       "      (blocks): Sequential(\n",
       "        (0): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Downsample2d(\n",
       "              (pool): AvgPool2dSame(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "              (expand): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(384, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2dSame(3072, 3072, kernel_size=(3, 3), stride=(2, 2), groups=3072, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): MaxxVitBlock(\n",
       "          (conv): MbConvBlock(\n",
       "            (shortcut): Identity()\n",
       "            (pre_norm): BatchNormAct2d(\n",
       "              768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (down): Identity()\n",
       "            (conv1_1x1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm1): BatchNormAct2d(\n",
       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (conv2_kxk): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
       "            (norm2): BatchNormAct2d(\n",
       "              3072, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
       "              (drop): Identity()\n",
       "              (act): GELUTanh()\n",
       "            )\n",
       "            (se): SEModule(\n",
       "              (fc1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (bn): Identity()\n",
       "              (act): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (gate): Sigmoid()\n",
       "            )\n",
       "            (conv3_1x1): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (attn_block): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (attn_grid): PartitionAttentionCl(\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): AttentionCl(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (rel_pos): RelPosBiasTf()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELUTanh()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): Identity()\n",
       "  (head): NormMlpClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "    (norm): LayerNorm2d((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (pre_logits): Sequential(\n",
       "      (fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (act): Tanh()\n",
       "    )\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f6229a5-cd92-4069-b8db-f8eba32f9beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "backbone_output = model(dummy_input)\n",
    "print(backbone_output.shape)\n",
    "backbone_dim = backbone_output.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc739f1c-d9b2-4cca-94f6-b02149bf70be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
