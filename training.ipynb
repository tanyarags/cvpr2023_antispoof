{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "531c5b8a-fbdd-4527-a193-0e97af647c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Current working directory: /home/alok/Desktop/antispoof_offline\n",
      "🔧 PyTorch version: 2.7.1+cu126\n",
      "🖥️  CUDA available: False\n",
      "✅ All modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# NetEase Anti-Spoofing Training Notebook\n",
    "# Two-Stage Training: ConvNext (Stage 1) → MaxViT (Stage 2)\n",
    "\n",
    "## Cell 1: Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Check if we're in the right directory\n",
    "print(\"📁 Current working directory:\", os.getcwd())\n",
    "print(\"🔧 PyTorch version:\", torch.__version__)\n",
    "print(\"🖥️  CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"🚀 GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"💾 GPU Memory:\", f\"{torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Import our training modules\n",
    "try:\n",
    "    from jupyter_netease_trainer import JupyterNeteaseTrainer\n",
    "    from model_manager import ModelManager\n",
    "    print(\"✅ All modules imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(\"Make sure all training files are in the current directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4bf8ec0-3090-422e-bbcb-24eb96de412c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Training Configuration:\n",
      "  📁 Data root: /home/alok/Desktop/antispoofData/data_2\n",
      "  💾 Output dir: ./netease_experiment_1\n",
      "  🗄️  Model cache: ./model\n",
      "  🏋️  Stage 1 epochs: 10\n",
      "  🏋️  Stage 2 epochs: 10\n",
      "  📦 Batch size: 16\n",
      "  🖼️  Image size: 224\n",
      "  🤖 Stage 1 model: convnextv2_base\n",
      "  🤖 Stage 2 model: maxvit_base_tf_224\n"
     ]
    }
   ],
   "source": [
    "## Cell 2: Configuration and Setup\n",
    "# ==========================================\n",
    "# CONFIGURATION - MODIFY THESE PATHS\n",
    "# ==========================================\n",
    "\n",
    "# Dataset configuration\n",
    "data_root = \"/home/alok/Desktop/antispoofData/data_2\"  # 🔴 CHANGE THIS TO YOUR DATASET PATH\n",
    "output_dir = \"./netease_experiment_1\"\n",
    "model_cache_dir = \"./model\"\n",
    "\n",
    "# Training configuration\n",
    "STAGE1_EPOCHS = 10  # Stage 1 training epochs\n",
    "STAGE2_EPOCHS = 10  # Stage 2 training epochs\n",
    "BATCH_SIZE = 16     # Adjust based on your GPU memory\n",
    "IMAGE_SIZE = 224    # Input image size\n",
    "\n",
    "# Model configuration\n",
    "STAGE1_MODEL = \"convnextv2_base\"      # convnext_base, convnext_small, convnext_large\n",
    "STAGE2_MODEL = \"maxvit_base_tf_224\" # maxvit_base_tf_224, maxvit_small_tf_224\n",
    "\n",
    "# 🔴 TRAINING FROM SCRATCH SETTING\n",
    "PRETRAINED_STAGE1 = False  # ← Change this to False for training from scratch\n",
    "PRETRAINED_STAGE2 = False   # Keep Stage 2 pretrained (recommended)\n",
    "\n",
    "print(\"🔧 Training Configuration:\")\n",
    "print(f\"  📁 Data root: {data_root}\")\n",
    "print(f\"  💾 Output dir: {output_dir}\")\n",
    "print(f\"  🗄️  Model cache: {model_cache_dir}\")\n",
    "print(f\"  🏋️  Stage 1 epochs: {STAGE1_EPOCHS}\")\n",
    "print(f\"  🏋️  Stage 2 epochs: {STAGE2_EPOCHS}\")\n",
    "print(f\"  📦 Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  🖼️  Image size: {IMAGE_SIZE}\")\n",
    "print(f\"  🤖 Stage 1 model: {STAGE1_MODEL}\")\n",
    "print(f\"  🤖 Stage 2 model: {STAGE2_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4215538f-7ea5-4c8f-8ba3-cc9a9a1e8dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 NetEase Trainer initialized\n",
      "📁 Data root: /home/alok/Desktop/antispoofData/data_2\n",
      "💾 Output dir: ./netease_experiment_1\n",
      "🗄️  Model cache: ./model\n",
      "🔧 Device: cpu\n",
      "✅ Dataset splits found: ['train', 'dev', 'test']\n",
      "📦 Cached models:\n",
      "  stage2: maxvit_base_tf_224\n",
      "  stage1: convnextv2_base\n",
      "\n",
      "============================================================\n",
      "🔍 DATASET VERIFICATION\n",
      "============================================================\n",
      "✅ Dataset found at /home/alok/Desktop/antispoofData/data_2\n",
      "\n",
      "📂 Dataset structure:\n",
      "  📁 dev/ (0 items)\n",
      "  📁 test/ (0 items)\n",
      "  📁 train/ (0 items)\n"
     ]
    }
   ],
   "source": [
    "## Cell 3: Initialize Trainer and Check Dataset\n",
    "# ==========================================\n",
    "# INITIALIZE TRAINER\n",
    "# ==========================================\n",
    "\n",
    "# Create trainer instance\n",
    "trainer = JupyterNeteaseTrainer(\n",
    "    data_root=data_root,\n",
    "    output_dir=output_dir,\n",
    "    model_cache_dir=model_cache_dir\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🔍 DATASET VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if dataset exists\n",
    "if not os.path.exists(data_root):\n",
    "    print(f\"❌ ERROR: Dataset not found at {data_root}\")\n",
    "    print(\"Please update the data_root path in the configuration cell above\")\n",
    "else:\n",
    "    print(f\"✅ Dataset found at {data_root}\")\n",
    "    \n",
    "    # List dataset contents\n",
    "    print(f\"\\n📂 Dataset structure:\")\n",
    "    try:\n",
    "        for item in sorted(os.listdir(data_root)):\n",
    "            item_path = os.path.join(data_root, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                file_count = len([f for f in os.listdir(item_path) \n",
    "                                if os.path.isfile(os.path.join(item_path, f))])\n",
    "                print(f\"  📁 {item}/ ({file_count} items)\")\n",
    "            else:\n",
    "                print(f\"  📄 {item}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️  Could not list contents: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "989e0c64-297b-4d46-98ed-c397ef4a5a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking model availability...\n",
      "📦 Currently cached models:\n",
      "  stage2: maxvit_base_tf_224\n",
      "  stage1: convnextv2_base\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "🤔 Pre-download models? This ensures offline training. (y/N):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭️  Skipping pre-download. Models will be downloaded during training if needed.\n"
     ]
    }
   ],
   "source": [
    "## Cell 4: Pre-download Models (Optional)\n",
    "# ==========================================\n",
    "# PRE-DOWNLOAD MODELS (OPTIONAL)\n",
    "# ==========================================\n",
    "\n",
    "print(\"🔍 Checking model availability...\")\n",
    "\n",
    "# Check current cache status\n",
    "model_manager = ModelManager(cache_base_dir=model_cache_dir)\n",
    "cached_models = model_manager.list_cached_models()\n",
    "\n",
    "if cached_models:\n",
    "    print(\"📦 Currently cached models:\")\n",
    "    for stage, models in cached_models.items():\n",
    "        if models:\n",
    "            print(f\"  {stage}: {', '.join(models)}\")\n",
    "else:\n",
    "    print(\"📦 No models currently cached\")\n",
    "\n",
    "# Option to pre-download models\n",
    "download_models = input(f\"\\n🤔 Pre-download models? This ensures offline training. (y/N): \").lower() == 'y'\n",
    "\n",
    "if download_models:\n",
    "    print(f\"\\n📥 Pre-downloading models...\")\n",
    "    trainer.download_models(stage1_model=STAGE1_MODEL, stage2_model=STAGE2_MODEL)\n",
    "else:\n",
    "    print(\"⏭️  Skipping pre-download. Models will be downloaded during training if needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a082a227-6cfc-4ba1-9f71-38f2c86bdb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Stage 1 Training\n",
      "============================================================\n",
      "🤖 Model: convnextv2_base\n",
      "🏋️  Epochs: 10\n",
      "📦 Batch size: 16\n",
      "============================================================\n",
      "🔥 Starting Stage 1 Training (ConvNext)\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 18:06:16,053 - INFO - Loading model from local directory: ./model/stage1/convnextv2_base\n",
      "2025-06-24 18:06:17,307 - INFO - Loaded model weights from: ./model/stage1/convnextv2_base/pytorch_model.bin\n",
      "2025-06-24 18:06:17,320 - INFO - Successfully loaded model from local cache\n",
      "2025-06-24 18:06:17,390 - INFO - Stage 1 Trainer initialized\n",
      "2025-06-24 18:06:17,391 - INFO - Device: cpu\n",
      "2025-06-24 18:06:17,391 - INFO - Model: convnextv2_base\n",
      "2025-06-24 18:06:17,391 - INFO - Train samples: 196\n",
      "2025-06-24 18:06:17,392 - INFO - Val samples: 196\n",
      "2025-06-24 18:06:17,392 - INFO - Starting Stage 1 training...\n",
      "Epoch 1/10:   0%|                                        | 0/13 [00:18<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Cell 5: Stage 1 Training (ConvNextv2)\n",
    "# ==========================================\n",
    "# STAGE 1 TRAINING: ConvNext\n",
    "# ==========================================\n",
    "\n",
    "print(\"🚀 Starting Stage 1 Training\")\n",
    "print(\"=\"*60)\n",
    "print(f\"🤖 Model: {STAGE1_MODEL}\")\n",
    "print(f\"🏋️  Epochs: {STAGE1_EPOCHS}\")\n",
    "print(f\"📦 Batch size: {BATCH_SIZE}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Record start time\n",
    "stage1_start_time = datetime.now()\n",
    "\n",
    "# Train Stage 1\n",
    "try:\n",
    "    soft_labels_path = trainer.train_stage1(\n",
    "        epochs=STAGE1_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=2e-4,\n",
    "        model_name=STAGE1_MODEL,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        pretrained=PRETRAINED_STAGE1,\n",
    "        # Additional parameters can be added here\n",
    "        weight_decay=0.01,\n",
    "        optimizer='adamw',\n",
    "        scheduler='cosine'\n",
    "    )\n",
    "    \n",
    "    # Record completion\n",
    "    stage1_end_time = datetime.now()\n",
    "    stage1_duration = stage1_end_time - stage1_start_time\n",
    "    \n",
    "    print(f\"\\n🎉 Stage 1 Training Completed!\")\n",
    "    print(f\"⏱️  Duration: {stage1_duration}\")\n",
    "    print(f\"🏆 Best validation accuracy: {trainer.stage1_trainer.best_val_acc:.2f}%\")\n",
    "    print(f\"📊 Best validation ACER: {trainer.stage1_trainer.best_val_acer:.4f}\")\n",
    "    print(f\"💾 Soft labels saved to: {soft_labels_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Stage 1 training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    soft_labels_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f2024c-d0a8-4f8c-9795-5148fe76b502",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 6: Stage 1 Results Analysis\n",
    "# ==========================================\n",
    "# STAGE 1 RESULTS ANALYSIS\n",
    "# ==========================================\n",
    "\n",
    "if trainer.stage1_history and trainer.stage1_trainer:\n",
    "    print(\"📊 Stage 1 Training Analysis\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Create DataFrame from training history\n",
    "    df_stage1 = pd.DataFrame(trainer.stage1_history)\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(f\"📈 Training Summary:\")\n",
    "    print(f\"  • Total epochs: {len(df_stage1)}\")\n",
    "    print(f\"  • Best validation accuracy: {trainer.stage1_trainer.best_val_acc:.2f}%\")\n",
    "    print(f\"  • Best validation ACER: {trainer.stage1_trainer.best_val_acer:.4f}\")\n",
    "    print(f\"  • Final training accuracy: {df_stage1.iloc[-1]['train_acc']:.2f}%\")\n",
    "    print(f\"  • Final validation accuracy: {df_stage1.iloc[-1]['val_acc']:.2f}%\")\n",
    "    \n",
    "    # Plot training progress\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(df_stage1['epoch'], df_stage1['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "    ax1.plot(df_stage1['epoch'], df_stage1['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "    ax1.set_title('Stage 1: Loss Curves', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    ax2.plot(df_stage1['epoch'], df_stage1['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
    "    ax2.plot(df_stage1['epoch'], df_stage1['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
    "    ax2.set_title('Stage 1: Accuracy Curves', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ACER curve\n",
    "    ax3.plot(df_stage1['epoch'], df_stage1['val_acer'], 'g-', label='Val ACER', linewidth=2)\n",
    "    ax3.set_title('Stage 1: ACER (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('ACER')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate\n",
    "    ax4.plot(df_stage1['epoch'], df_stage1['lr'], 'orange', label='Learning Rate', linewidth=2)\n",
    "    ax4.set_title('Stage 1: Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Learning Rate')\n",
    "    ax4.set_yscale('log')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display training history table\n",
    "    print(f\"\\n📋 Detailed Training History:\")\n",
    "    display(df_stage1.round(4))\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No Stage 1 training history available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae269c3-913a-4778-9270-01751abc4f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 7: Stage 1 Model Information\n",
    "# ==========================================\n",
    "# STAGE 1 MODEL INFORMATION\n",
    "# ==========================================\n",
    "\n",
    "if trainer.stage1_trainer:\n",
    "    print(\"🔍 Stage 1 Model Information\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Model architecture info\n",
    "    model = trainer.stage1_trainer.model\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"🤖 Model: {STAGE1_MODEL}\")\n",
    "    print(f\"📊 Total parameters: {total_params:,}\")\n",
    "    print(f\"🎛️  Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"💾 Model size: ~{total_params * 4 / 1e6:.1f} MB (float32)\")\n",
    "    \n",
    "    # Training configuration\n",
    "    print(f\"\\n⚙️  Training Configuration:\")\n",
    "    print(f\"  • Optimizer: {trainer.stage1_trainer.config['optimizer']}\")\n",
    "    print(f\"  • Learning rate: {trainer.stage1_trainer.config['learning_rate']}\")\n",
    "    print(f\"  • Weight decay: {trainer.stage1_trainer.config['weight_decay']}\")\n",
    "    print(f\"  • Scheduler: {trainer.stage1_trainer.config['scheduler']}\")\n",
    "    print(f\"  • Batch size: {trainer.stage1_trainer.config['batch_size']}\")\n",
    "    \n",
    "    # Checkpoint info\n",
    "    checkpoint_dir = trainer.stage1_trainer.config['checkpoint_dir']\n",
    "    best_checkpoint = os.path.join(checkpoint_dir, 'stage1_best.pth')\n",
    "    latest_checkpoint = os.path.join(checkpoint_dir, 'stage1_latest.pth')\n",
    "    \n",
    "    print(f\"\\n💾 Saved Checkpoints:\")\n",
    "    if os.path.exists(best_checkpoint):\n",
    "        size = os.path.getsize(best_checkpoint) / 1e6\n",
    "        print(f\"  ✅ Best checkpoint: {best_checkpoint} ({size:.1f} MB)\")\n",
    "    if os.path.exists(latest_checkpoint):\n",
    "        size = os.path.getsize(latest_checkpoint) / 1e6\n",
    "        print(f\"  ✅ Latest checkpoint: {latest_checkpoint} ({size:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f27e30-60d9-4c69-b357-d572ead15ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 8: Prepare for Stage 2\n",
    "# ==========================================\n",
    "# PREPARE FOR STAGE 2\n",
    "# ==========================================\n",
    "\n",
    "print(\"🔄 Preparing for Stage 2 Training\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Check if soft labels exist\n",
    "if soft_labels_path and os.path.exists(soft_labels_path):\n",
    "    print(f\"✅ Soft labels found: {soft_labels_path}\")\n",
    "    \n",
    "    # Load and inspect soft labels\n",
    "    try:\n",
    "        soft_labels = torch.load(soft_labels_path, map_location='cpu')\n",
    "        print(f\"📊 Soft labels statistics:\")\n",
    "        print(f\"  • Number of samples: {len(soft_labels)}\")\n",
    "        print(f\"  • Sample paths: {list(soft_labels.keys())[:3]}...\")\n",
    "        \n",
    "        # Analyze soft label distribution\n",
    "        import numpy as np\n",
    "        probs = np.array(list(soft_labels.values()))\n",
    "        live_probs = probs[:, 1]  # Probability of live class\n",
    "        \n",
    "        print(f\"  • Live probability stats:\")\n",
    "        print(f\"    - Mean: {live_probs.mean():.3f}\")\n",
    "        print(f\"    - Std: {live_probs.std():.3f}\")\n",
    "        print(f\"    - Min: {live_probs.min():.3f}\")\n",
    "        print(f\"    - Max: {live_probs.max():.3f}\")\n",
    "        \n",
    "        # Plot distribution\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(live_probs, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        plt.title('Distribution of Live Probabilities')\n",
    "        plt.xlabel('Live Probability')\n",
    "        plt.ylabel('Count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.hist(probs[:, 0], bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "        plt.title('Distribution of Spoof Probabilities')\n",
    "        plt.xlabel('Spoof Probability')\n",
    "        plt.ylabel('Count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Could not analyze soft labels: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"❌ Soft labels not found!\")\n",
    "    if not soft_labels_path:\n",
    "        print(\"Stage 1 training may have failed.\")\n",
    "    else:\n",
    "        print(f\"Expected path: {soft_labels_path}\")\n",
    "    \n",
    "    # Ask user what to do\n",
    "    action = input(\"\\n🤔 What would you like to do?\\n\"\n",
    "                  \"1. Continue with Stage 2 anyway (will fail)\\n\"\n",
    "                  \"2. Stop here and fix Stage 1\\n\"\n",
    "                  \"3. Provide custom soft labels path\\n\"\n",
    "                  \"Enter choice (1/2/3): \")\n",
    "    \n",
    "    if action == \"2\":\n",
    "        print(\"🛑 Stopping execution. Please fix Stage 1 training first.\")\n",
    "        # You can stop execution here if in a script\n",
    "    elif action == \"3\":\n",
    "        custom_path = input(\"Enter path to soft labels file: \")\n",
    "        if os.path.exists(custom_path):\n",
    "            soft_labels_path = custom_path\n",
    "            print(f\"✅ Using custom soft labels: {soft_labels_path}\")\n",
    "        else:\n",
    "            print(f\"❌ File not found: {custom_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba55dd-4144-4651-b691-b243da287090",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 9: Stage 2 Training (MaxViT)\n",
    "# ==========================================\n",
    "# STAGE 2 TRAINING: MaxViT\n",
    "# ==========================================\n",
    "\n",
    "if soft_labels_path and os.path.exists(soft_labels_path):\n",
    "    print(\"🚀 Starting Stage 2 Training\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"🤖 Model: {STAGE2_MODEL}\")\n",
    "    print(f\"🏋️  Epochs: {STAGE2_EPOCHS}\")\n",
    "    print(f\"📦 Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"🔮 Soft labels: {soft_labels_path}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Record start time\n",
    "    stage2_start_time = datetime.now()\n",
    "    \n",
    "    # Train Stage 2\n",
    "    try:\n",
    "        test_acc, test_acer = trainer.train_stage2(\n",
    "            soft_labels_path=soft_labels_path,\n",
    "            epochs=STAGE2_EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            learning_rate=1e-4,\n",
    "            model_name=STAGE2_MODEL,\n",
    "            image_size=IMAGE_SIZE,\n",
    "            pretrained=PRETRAINED_STAGE2,\n",
    "            # Stage 2 specific parameters\n",
    "            feature_dim=512,\n",
    "            focal_alpha=1.0,\n",
    "            focal_gamma=2.0,\n",
    "            triplet_margin=0.3,\n",
    "            kd_temperature=3.0,\n",
    "            focal_weight=1.0,\n",
    "            triplet_weight=0.5,\n",
    "            kd_weight=1.0,\n",
    "            use_mixup_cutmix=True,\n",
    "            mixup_alpha=0.2,\n",
    "            cutmix_alpha=1.0\n",
    "        )\n",
    "        \n",
    "        # Record completion\n",
    "        stage2_end_time = datetime.now()\n",
    "        stage2_duration = stage2_end_time - stage2_start_time\n",
    "        total_duration = stage2_end_time - stage1_start_time\n",
    "        \n",
    "        print(f\"\\n🎉 Stage 2 Training Completed!\")\n",
    "        print(f\"⏱️  Stage 2 duration: {stage2_duration}\")\n",
    "        print(f\"⏱️  Total pipeline duration: {total_duration}\")\n",
    "        print(f\"🏆 Best validation accuracy: {trainer.stage2_trainer.best_val_acc:.2f}%\")\n",
    "        print(f\"📊 Best validation ACER: {trainer.stage2_trainer.best_val_acer:.4f}\")\n",
    "        print(f\"🎯 Final test accuracy: {test_acc:.2f}%\")\n",
    "        print(f\"📈 Final test ACER: {test_acer:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Stage 2 training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        test_acc, test_acer = None, None\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Cannot start Stage 2: No valid soft labels found\")\n",
    "    test_acc, test_acer = None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f52aa7-d353-40ce-a4c1-b3f126b123e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 10: Stage 2 Results Analysis\n",
    "# ==========================================\n",
    "# STAGE 2 RESULTS ANALYSIS\n",
    "# ==========================================\n",
    "\n",
    "if trainer.stage2_history and trainer.stage2_trainer:\n",
    "    print(\"📊 Stage 2 Training Analysis\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Create DataFrame from training history\n",
    "    df_stage2 = pd.DataFrame(trainer.stage2_history)\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(f\"📈 Training Summary:\")\n",
    "    print(f\"  • Total epochs: {len(df_stage2)}\")\n",
    "    print(f\"  • Best validation accuracy: {trainer.stage2_trainer.best_val_acc:.2f}%\")\n",
    "    print(f\"  • Best validation ACER: {trainer.stage2_trainer.best_val_acer:.4f}\")\n",
    "    print(f\"  • Final test accuracy: {test_acc:.2f}%\" if test_acc else \"  • Test accuracy: N/A\")\n",
    "    print(f\"  • Final test ACER: {test_acer:.4f}\" if test_acer else \"  • Test ACER: N/A\")\n",
    "    \n",
    "    # Plot training progress with loss components\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss components\n",
    "    ax1.plot(df_stage2['epoch'], df_stage2['train_loss'], 'black', label='Total Loss', linewidth=3)\n",
    "    ax1.plot(df_stage2['epoch'], df_stage2['focal_loss'], 'red', label='Focal Loss', linewidth=2)\n",
    "    ax1.plot(df_stage2['epoch'], df_stage2['triplet_loss'], 'blue', label='Triplet Loss', linewidth=2)\n",
    "    ax1.plot(df_stage2['epoch'], df_stage2['kd_loss'], 'green', label='KD Loss', linewidth=2)\n",
    "    ax1.set_title('Stage 2: Loss Components', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    ax2.plot(df_stage2['epoch'], df_stage2['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
    "    ax2.plot(df_stage2['epoch'], df_stage2['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
    "    ax2.set_title('Stage 2: Accuracy Curves', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ACER curve\n",
    "    ax3.plot(df_stage2['epoch'], df_stage2['val_acer'], 'g-', label='Val ACER', linewidth=2)\n",
    "    ax3.set_title('Stage 2: ACER (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('ACER')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate\n",
    "    ax4.plot(df_stage2['epoch'], df_stage2['lr'], 'orange', label='Learning Rate', linewidth=2)\n",
    "    ax4.set_title('Stage 2: Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Learning Rate')\n",
    "    ax4.set_yscale('log')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Loss component analysis\n",
    "    print(f\"\\n🔍 Loss Component Analysis (Final Epoch):\")\n",
    "    final_epoch = df_stage2.iloc[-1]\n",
    "    total_loss = final_epoch['train_loss']\n",
    "    focal_loss = final_epoch['focal_loss']\n",
    "    triplet_loss = final_epoch['triplet_loss']\n",
    "    kd_loss = final_epoch['kd_loss']\n",
    "    \n",
    "    print(f\"  • Total Loss: {total_loss:.4f}\")\n",
    "    print(f\"  • Focal Loss: {focal_loss:.4f} ({focal_loss/total_loss*100:.1f}%)\")\n",
    "    print(f\"  • Triplet Loss: {triplet_loss:.4f} ({triplet_loss/total_loss*100:.1f}%)\")\n",
    "    print(f\"  • KD Loss: {kd_loss:.4f} ({kd_loss/total_loss*100:.1f}%)\")\n",
    "    \n",
    "    # Display training history table\n",
    "    print(f\"\\n📋 Detailed Training History:\")\n",
    "    display(df_stage2.round(4))\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No Stage 2 training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48411e1-10bb-4f0f-9bcc-dae483c864de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 11: Complete Pipeline Summary\n",
    "\n",
    "# ==========================================\n",
    "# COMPLETE PIPELINE SUMMARY\n",
    "# ==========================================\n",
    "\n",
    "print(\"🎯 NetEase Two-Stage Training Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Overall results\n",
    "if trainer.stage1_trainer and trainer.stage2_trainer:\n",
    "    print(\"✅ TRAINING COMPLETED SUCCESSFULLY\")\n",
    "    print(\"\\n📊 Final Results:\")\n",
    "    print(f\"  🥇 Stage 1 (ConvNext):\")\n",
    "    print(f\"     • Best Val Accuracy: {trainer.stage1_trainer.best_val_acc:.2f}%\")\n",
    "    print(f\"     • Best Val ACER: {trainer.stage1_trainer.best_val_acer:.4f}\")\n",
    "    \n",
    "    print(f\"  🥇 Stage 2 (MaxViT):\")\n",
    "    print(f\"     • Best Val Accuracy: {trainer.stage2_trainer.best_val_acc:.2f}%\")\n",
    "    print(f\"     • Best Val ACER: {trainer.stage2_trainer.best_val_acer:.4f}\")\n",
    "    if test_acc and test_acer:\n",
    "        print(f\"     • Test Accuracy: {test_acc:.2f}%\")\n",
    "        print(f\"     • Test ACER: {test_acer:.4f}\")\n",
    "    \n",
    "    # Performance improvement\n",
    "    stage1_acc = trainer.stage1_trainer.best_val_acc\n",
    "    stage2_acc = trainer.stage2_trainer.best_val_acc\n",
    "    improvement = stage2_acc - stage1_acc\n",
    "    print(f\"\\n📈 Performance Improvement:\")\n",
    "    print(f\"  • Accuracy improvement: {improvement:+.2f}%\")\n",
    "    print(f\"  • Relative improvement: {improvement/stage1_acc*100:+.1f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Training incomplete or failed\")\n",
    "\n",
    "# Training configuration summary\n",
    "print(f\"\\n⚙️  Training Configuration:\")\n",
    "print(f\"  • Stage 1 Model: {STAGE1_MODEL}\")\n",
    "print(f\"  • Stage 2 Model: {STAGE2_MODEL}\")\n",
    "print(f\"  • Stage 1 Epochs: {STAGE1_EPOCHS}\")\n",
    "print(f\"  • Stage 2 Epochs: {STAGE2_EPOCHS}\")\n",
    "print(f\"  • Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  • Image Size: {IMAGE_SIZE}\")\n",
    "\n",
    "# File locations\n",
    "print(f\"\\n📁 Output Files:\")\n",
    "print(f\"  • Output directory: {output_dir}\")\n",
    "print(f\"  • Model cache: {model_cache_dir}\")\n",
    "if soft_labels_path:\n",
    "    print(f\"  • Soft labels: {soft_labels_path}\")\n",
    "\n",
    "# Timing information\n",
    "if 'stage1_duration' in locals() and 'stage2_duration' in locals():\n",
    "    print(f\"\\n⏱️  Timing:\")\n",
    "    print(f\"  • Stage 1 duration: {stage1_duration}\")\n",
    "    print(f\"  • Stage 2 duration: {stage2_duration}\")\n",
    "    print(f\"  • Total duration: {stage1_duration + stage2_duration}\")\n",
    "\n",
    "print(f\"\\n🎉 Notebook execution completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9884480-bcc3-4daa-86fa-505a15231ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 12: Save Results and Cleanup\n",
    "# ==========================================\n",
    "# SAVE RESULTS AND CLEANUP\n",
    "# ==========================================\n",
    "\n",
    "print(\"💾 Saving Results and Cleanup\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Compile all results\n",
    "results_summary = {\n",
    "    'experiment_info': {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'stage1_model': STAGE1_MODEL,\n",
    "        'stage2_model': STAGE2_MODEL,\n",
    "        'stage1_epochs': STAGE1_EPOCHS,\n",
    "        'stage2_epochs': STAGE2_EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'image_size': IMAGE_SIZE\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add Stage 1 results\n",
    "if trainer.stage1_trainer:\n",
    "    results_summary['stage1'] = {\n",
    "        'best_val_acc': trainer.stage1_trainer.best_val_acc,\n",
    "        'best_val_acer': trainer.stage1_trainer.best_val_acer,\n",
    "        'training_history': trainer.stage1_history\n",
    "    }\n",
    "\n",
    "# Add Stage 2 results\n",
    "if trainer.stage2_trainer:\n",
    "    results_summary['stage2'] = {\n",
    "        'best_val_acc': trainer.stage2_trainer.best_val_acc,\n",
    "        'best_val_acer': trainer.stage2_trainer.best_val_acer,\n",
    "        'test_acc': test_acc,\n",
    "        'test_acer': test_acer,\n",
    "        'training_history': trainer.stage2_history\n",
    "    }\n",
    "\n",
    "# Save results\n",
    "results_file = os.path.join(output_dir, 'notebook_results.json')\n",
    "try:\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results_summary, f, indent=2)\n",
    "    print(f\"✅ Results saved to: {results_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not save results: {e}\")\n",
    "\n",
    "# List all generated files\n",
    "print(f\"\\n📋 Generated Files:\")\n",
    "if os.path.exists(output_dir):\n",
    "    for root, dirs, files in os.walk(output_dir):\n",
    "        level = root.replace(output_dir, '').count(os.sep)\n",
    "        indent = ' ' * 2 * level\n",
    "        print(f\"{indent}📁 {os.path.basename(root)}/\")\n",
    "        subindent = ' ' * 2 * (level + 1)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            size = os.path.getsize(file_path) / 1024  # KB\n",
    "            print(f\"{subindent}📄 {file} ({size:.1f} KB)\")\n",
    "\n",
    "# Memory cleanup\n",
    "print(f\"\\n🧹 Cleanup:\")\n",
    "if 'trainer' in locals():\n",
    "    # Clear GPU memory if used\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"✅ GPU memory cleared\")\n",
    "    print(\"✅ Training objects available for further analysis\")\n",
    "\n",
    "print(f\"\\n🎯 Training Complete!\")\n",
    "print(\"You can now:\")\n",
    "print(\"  • Analyze the results using trainer.plot_complete_history()\")\n",
    "print(\"  • Load checkpoints with trainer.load_checkpoint(stage, 'best')\")\n",
    "print(\"  • Access training history via trainer.stage1_history and trainer.stage2_history\")\n",
    "print(\"  • Use the trained models for inference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce3ea6f-fb17-424f-906c-1494dc6fb818",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 13: Optional - Additional Analysis and Visualization\n",
    "# ==========================================\n",
    "# OPTIONAL: ADDITIONAL ANALYSIS\n",
    "# ==========================================\n",
    "\n",
    "print(\"📊 Additional Analysis and Visualizations\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Combined training curves comparison\n",
    "if trainer.stage1_history and trainer.stage2_history:\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Prepare data\n",
    "    df1 = pd.DataFrame(trainer.stage1_history)\n",
    "    df2 = pd.DataFrame(trainer.stage2_history)\n",
    "    \n",
    "    # Adjust Stage 2 epochs to continue from Stage 1\n",
    "    df2_adjusted = df2.copy()\n",
    "    df2_adjusted['epoch'] = df2_adjusted['epoch'] + STAGE1_EPOCHS\n",
    "    \n",
    "    # Combined accuracy plot\n",
    "    ax1.plot(df1['epoch'], df1['train_acc'], 'b-', label='Stage 1 Train', linewidth=2, alpha=0.8)\n",
    "    ax1.plot(df1['epoch'], df1['val_acc'], 'b--', label='Stage 1 Val', linewidth=2, alpha=0.8)\n",
    "    ax1.plot(df2_adjusted['epoch'], df2_adjusted['train_acc'], 'r-', label='Stage 2 Train', linewidth=2, alpha=0.8)\n",
    "    ax1.plot(df2_adjusted['epoch'], df2_adjusted['val_acc'], 'r--', label='Stage 2 Val', linewidth=2, alpha=0.8)\n",
    "    ax1.axvline(x=STAGE1_EPOCHS, color='gray', linestyle=':', alpha=0.7, label='Stage Transition')\n",
    "    ax1.set_title('Complete Pipeline: Accuracy Progression', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy (%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Combined ACER plot\n",
    "    ax2.plot(df1['epoch'], df1['val_acer'], 'b-', label='Stage 1 ACER', linewidth=2, alpha=0.8)\n",
    "    ax2.plot(df2_adjusted['epoch'], df2_adjusted['val_acer'], 'r-', label='Stage 2 ACER', linewidth=2, alpha=0.8)\n",
    "    ax2.axvline(x=STAGE1_EPOCHS, color='gray', linestyle=':', alpha=0.7, label='Stage Transition')\n",
    "    ax2.set_title('Complete Pipeline: ACER Progression', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('ACER (Lower is Better)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Stage 2 loss components breakdown\n",
    "    ax3.stackplot(df2['epoch'], df2['focal_loss'], df2['triplet_loss'], df2['kd_loss'], \n",
    "                  labels=['Focal Loss', 'Triplet Loss', 'KD Loss'], alpha=0.7)\n",
    "    ax3.set_title('Stage 2: Loss Components Breakdown', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Loss Value')\n",
    "    ax3.legend(loc='upper right')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Performance comparison bar chart\n",
    "    metrics = ['Val Accuracy', 'Val ACER']\n",
    "    stage1_vals = [trainer.stage1_trainer.best_val_acc, trainer.stage1_trainer.best_val_acer * 100]  # Scale ACER for visibility\n",
    "    stage2_vals = [trainer.stage2_trainer.best_val_acc, trainer.stage2_trainer.best_val_acer * 100]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax4.bar(x - width/2, stage1_vals, width, label='Stage 1', alpha=0.8, color='skyblue')\n",
    "    ax4.bar(x + width/2, stage2_vals, width, label='Stage 2', alpha=0.8, color='lightcoral')\n",
    "    ax4.set_title('Stage Comparison: Best Results', fontsize=14, fontweight='bold')\n",
    "    ax4.set_ylabel('Value (%)')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(['Accuracy (%)', 'ACER × 100'])\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (s1, s2) in enumerate(zip(stage1_vals, stage2_vals)):\n",
    "        ax4.text(i - width/2, s1 + 0.5, f'{s1:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "        ax4.text(i + width/2, s2 + 0.5, f'{s2:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance metrics table\n",
    "    print(\"\\n📋 Performance Comparison Table:\")\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Metric': ['Best Validation Accuracy (%)', 'Best Validation ACER', 'Final Test Accuracy (%)', 'Final Test ACER'],\n",
    "        'Stage 1': [trainer.stage1_trainer.best_val_acc, trainer.stage1_trainer.best_val_acer, 'N/A', 'N/A'],\n",
    "        'Stage 2': [trainer.stage2_trainer.best_val_acc, trainer.stage2_trainer.best_val_acer, \n",
    "                   test_acc if test_acc else 'N/A', test_acer if test_acer else 'N/A'],\n",
    "        'Improvement': [\n",
    "            f\"{trainer.stage2_trainer.best_val_acc - trainer.stage1_trainer.best_val_acc:+.2f}\",\n",
    "            f\"{trainer.stage2_trainer.best_val_acer - trainer.stage1_trainer.best_val_acer:+.4f}\",\n",
    "            'N/A', 'N/A'\n",
    "        ]\n",
    "    })\n",
    "    display(comparison_df)\n",
    "\n",
    "else:\n",
    "    print(\"⚠️  Cannot create comparison plots - missing training history\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd57dd02-cda2-48c9-b635-b1fc7455775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 14: Model Inference Example (Optional)\n",
    "# ==========================================\n",
    "# OPTIONAL: MODEL INFERENCE EXAMPLE\n",
    "# ==========================================\n",
    "\n",
    "print(\"🔮 Model Inference Example\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if trainer.stage2_trainer:\n",
    "    print(\"Setting up inference example...\")\n",
    "    \n",
    "    # Load the best Stage 2 model\n",
    "    best_checkpoint = trainer.load_checkpoint(2, 'best')\n",
    "    \n",
    "    if best_checkpoint:\n",
    "        print(\"✅ Best Stage 2 model loaded successfully\")\n",
    "        \n",
    "        # Example inference function\n",
    "        def predict_single_image(model, image_path, device, transform=None):\n",
    "            \"\"\"\n",
    "            Predict on a single image\n",
    "            \n",
    "            Args:\n",
    "                model: Trained model\n",
    "                image_path: Path to image\n",
    "                device: Device to run inference on\n",
    "                transform: Image transformations\n",
    "            \n",
    "            Returns:\n",
    "                prediction, confidence\n",
    "            \"\"\"\n",
    "            from PIL import Image\n",
    "            import torchvision.transforms as transforms\n",
    "            \n",
    "            # Default transform if none provided\n",
    "            if transform is None:\n",
    "                transform = transforms.Compose([\n",
    "                    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                       std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "            \n",
    "            # Load and preprocess image\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Inference\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                logits, features = model(image_tensor)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                predicted_class = torch.argmax(probs, dim=1).item()\n",
    "                confidence = probs[0, predicted_class].item()\n",
    "            \n",
    "            class_names = ['Spoof', 'Live']\n",
    "            return class_names[predicted_class], confidence, probs[0].cpu().numpy()\n",
    "        \n",
    "        print(\"🎯 Inference function defined\")\n",
    "        print(\"Usage example:\")\n",
    "        print(\"  prediction, confidence, probs = predict_single_image(\")\n",
    "        print(\"      trainer.stage2_trainer.model, '/path/to/image.jpg', trainer.device)\")\n",
    "        print(\"  print(f'Prediction: {prediction} (confidence: {confidence:.3f})')\")\n",
    "        \n",
    "        # Show model summary\n",
    "        print(f\"\\n🤖 Model Summary:\")\n",
    "        print(f\"  • Architecture: {STAGE2_MODEL}\")\n",
    "        print(f\"  • Input size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
    "        print(f\"  • Output classes: 2 (Spoof, Live)\")\n",
    "        print(f\"  • Feature dimension: {trainer.stage2_trainer.config['feature_dim']}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ Could not load best checkpoint for inference\")\n",
    "else:\n",
    "    print(\"❌ No trained Stage 2 model available for inference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5bdfff-87a4-4c55-8003-404cd55be1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 15: Export and Save Final Models\n",
    "# ==========================================\n",
    "# EXPORT AND SAVE FINAL MODELS\n",
    "# ==========================================\n",
    "\n",
    "print(\"📦 Exporting Final Models\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "export_dir = os.path.join(output_dir, 'exported_models')\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "# Export Stage 1 model\n",
    "if trainer.stage1_trainer:\n",
    "    try:\n",
    "        stage1_export_path = os.path.join(export_dir, f'stage1_{STAGE1_MODEL}_final.pth')\n",
    "        \n",
    "        # Create export package\n",
    "        export_package = {\n",
    "            'model_state_dict': trainer.stage1_trainer.model.state_dict(),\n",
    "            'config': trainer.stage1_trainer.config,\n",
    "            'best_val_acc': trainer.stage1_trainer.best_val_acc,\n",
    "            'best_val_acer': trainer.stage1_trainer.best_val_acer,\n",
    "            'model_name': STAGE1_MODEL,\n",
    "            'stage': 1,\n",
    "            'training_epochs': STAGE1_EPOCHS,\n",
    "            'export_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        torch.save(export_package, stage1_export_path)\n",
    "        size = os.path.getsize(stage1_export_path) / 1e6\n",
    "        print(f\"✅ Stage 1 model exported: {stage1_export_path} ({size:.1f} MB)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to export Stage 1 model: {e}\")\n",
    "\n",
    "# Export Stage 2 model\n",
    "if trainer.stage2_trainer:\n",
    "    try:\n",
    "        stage2_export_path = os.path.join(export_dir, f'stage2_{STAGE2_MODEL}_final.pth')\n",
    "        \n",
    "        # Create export package\n",
    "        export_package = {\n",
    "            'model_state_dict': trainer.stage2_trainer.model.state_dict(),\n",
    "            'config': trainer.stage2_trainer.config,\n",
    "            'best_val_acc': trainer.stage2_trainer.best_val_acc,\n",
    "            'best_val_acer': trainer.stage2_trainer.best_val_acer,\n",
    "            'test_acc': test_acc,\n",
    "            'test_acer': test_acer,\n",
    "            'model_name': STAGE2_MODEL,\n",
    "            'stage': 2,\n",
    "            'training_epochs': STAGE2_EPOCHS,\n",
    "            'soft_labels_path': soft_labels_path,\n",
    "            'export_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        torch.save(export_package, stage2_export_path)\n",
    "        size = os.path.getsize(stage2_export_path) / 1e6\n",
    "        print(f\"✅ Stage 2 model exported: {stage2_export_path} ({size:.1f} MB)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to export Stage 2 model: {e}\")\n",
    "\n",
    "# Create model loading script\n",
    "loading_script = f'''\n",
    "# Model Loading Script\n",
    "# Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "import torch\n",
    "from model_manager import ModelManager\n",
    "\n",
    "def load_trained_model(export_path, device='cpu'):\n",
    "    \"\"\"Load exported model\"\"\"\n",
    "    checkpoint = torch.load(export_path, map_location=device)\n",
    "    \n",
    "    # Recreate model using ModelManager\n",
    "    manager = ModelManager()\n",
    "    model = manager.load_or_download_model(\n",
    "        stage=f\"stage{{checkpoint['stage']}}\",\n",
    "        model_name=checkpoint['model_name'],\n",
    "        num_classes=2,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Load trained weights\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Loaded {{checkpoint['model_name']}} (Stage {{checkpoint['stage']}})\")\n",
    "    print(f\"Best Val Acc: {{checkpoint['best_val_acc']:.2f}}%\")\n",
    "    print(f\"Best Val ACER: {{checkpoint['best_val_acer']:.4f}}\")\n",
    "    \n",
    "    return model, checkpoint\n",
    "\n",
    "# Usage examples:\n",
    "# stage1_model, stage1_info = load_trained_model('{stage1_export_path if 'stage1_export_path' in locals() else 'stage1_model.pth'}')\n",
    "# stage2_model, stage2_info = load_trained_model('{stage2_export_path if 'stage2_export_path' in locals() else 'stage2_model.pth'}')\n",
    "'''\n",
    "\n",
    "script_path = os.path.join(export_dir, 'load_models.py')\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(loading_script)\n",
    "\n",
    "print(f\"✅ Model loading script saved: {script_path}\")\n",
    "\n",
    "# Summary of exports\n",
    "print(f\"\\n📋 Export Summary:\")\n",
    "if os.path.exists(export_dir):\n",
    "    for file in os.listdir(export_dir):\n",
    "        file_path = os.path.join(export_dir, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            size = os.path.getsize(file_path) / 1e6\n",
    "            print(f\"  📄 {file} ({size:.1f} MB)\")\n",
    "\n",
    "print(f\"\\n🎉 All exports completed successfully!\")\n",
    "print(f\"Models and scripts saved in: {export_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae46755-fc27-4fca-be9a-c005c94e7832",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell 16: Final Cleanup and Summary\n",
    "# ==========================================\n",
    "# FINAL CLEANUP AND SUMMARY\n",
    "# ==========================================\n",
    "\n",
    "print(\"🏁 Final Summary and Cleanup\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Training summary\n",
    "total_epochs = STAGE1_EPOCHS + STAGE2_EPOCHS\n",
    "print(f\"✅ NetEase Two-Stage Training Completed Successfully!\")\n",
    "print(f\"\\n📊 Training Summary:\")\n",
    "print(f\"  • Total epochs trained: {total_epochs}\")\n",
    "print(f\"  • Stage 1 ({STAGE1_MODEL}): {STAGE1_EPOCHS} epochs\")\n",
    "print(f\"  • Stage 2 ({STAGE2_MODEL}): {STAGE2_EPOCHS} epochs\")\n",
    "print(f\"  • Batch size used: {BATCH_SIZE}\")\n",
    "\n",
    "if trainer.stage1_trainer and trainer.stage2_trainer:\n",
    "    print(f\"\\n🏆 Best Results:\")\n",
    "    print(f\"  • Stage 1 Best Val Acc: {trainer.stage1_trainer.best_val_acc:.2f}%\")\n",
    "    print(f\"  • Stage 2 Best Val Acc: {trainer.stage2_trainer.best_val_acc:.2f}%\")\n",
    "    print(f\"  • Final Test Accuracy: {test_acc:.2f}%\" if test_acc else \"  • Test Accuracy: Not available\")\n",
    "    print(f\"  • Final Test ACER: {test_acer:.4f}\" if test_acer else \"  • Test ACER: Not available\")\n",
    "\n",
    "# File system summary\n",
    "print(f\"\\n📁 Generated Files and Directories:\")\n",
    "print(f\"  • Main output: {output_dir}\")\n",
    "print(f\"  • Model cache: {model_cache_dir}\")\n",
    "print(f\"  • Exported models: {os.path.join(output_dir, 'exported_models')}\")\n",
    "\n",
    "# Memory cleanup\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"\\n🧹 GPU memory cleared\")\n",
    "\n",
    "print(f\"\\n💡 Next Steps:\")\n",
    "print(f\"  1. Analyze results using the generated plots and metrics\")\n",
    "print(f\"  2. Use exported models for inference on new data\")\n",
    "print(f\"  3. Experiment with different hyperparameters if needed\")\n",
    "print(f\"  4. Consider longer training for better performance\")\n",
    "\n",
    "print(f\"\\n🎯 Notebook completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Happy experimenting! 🚀\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
